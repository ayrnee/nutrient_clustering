#!/usr/bin/env python

import os
import numpy as np
import pylab as pl
from sklearn.cluster import KMeans
from sklearn.preprocessing import MinMaxScaler

def randIndex(truth, predicted):
	"""
	The function is to measure similarity between two label assignments
	truth: ground truth labels for the dataset (1 x 1496)
	predicted: predicted labels (1 x 1496)
	"""
	if len(truth) != len(predicted):
		print "different sizes of the label assignments"
		return -1
	elif (len(truth) == 1):
		return 1
	sizeLabel = len(truth)
	agree_same = 0
	disagree_same = 0
	count = 0
	for i in range(sizeLabel-1):
		for j in range(i+1,sizeLabel):
			if ((truth[i] == truth[j]) and (predicted[i] == predicted[j])):
				agree_same += 1
			elif ((truth[i] != truth[j]) and (predicted[i] != predicted[j])):
				disagree_same +=1
			count += 1
	return (agree_same+disagree_same)/float(count)

def read_file(f_name, subdir = 'dat/'):
    samples = []
    names = []
    with open('./data/' + subdir + f_name) as file:
        for line in file:
            raw = line.strip().split('^')[:-1]
            names.append(raw[:1])
            samples.append(map(float, raw[1:]))
    return names, samples

def read_data(dat_dir):
    combined_data = [[], [], []]
    seperated_data = []
    label = 0
    for file in os.listdir(dat_dir):
        print file
        temp_lists = read_file(file)
        combined_data[0] += temp_lists[0]
        combined_data[1] += temp_lists[1]
        combined_data[2] += [label]*len(temp_lists[0])
        seperated_data.append(np.array(temp_lists[1]))
        label += 1
    data = [np.array(combined_data[0]), np.array(combined_data[1]), np.array(combined_data[2])]
    return data, seperated_data

def normalize_vals(data):
    scaler = MinMaxScaler()
    return scaler.fit_transform(data[1])

def k_means(data):
    kms = KMeans(n_clusters = 4, n_jobs =  -1)
    return kms.fit_predict(data[1])

def k_means_centroids(data):
    kms = KMeans(
        n_clusters = 4,
        init = 'random',
        n_init = 1,
        n_jobs =  -1
    )
    kms.fit(data)
    return kms.inertia_, kms.labels_

def cluster_test(data):
    start = 0
    samples = []
    datalabels = []
    print "hi"
# print "a more aggressive hi"


    fig = pl.figure
    hClsMat = sch.linkage(samples, method='complete') # Complete clustering
    sch.dendrogram(hClsMat, labels= datalabels, leaf_rotation = 45)
    fig.savefig("thing.pdf")






def main():
    data, sep_data = read_data('./data/dat')
    # print data[1].shape
    data[1] = normalize_vals(data)
    # for sample in data:
    #     print sample
    kms_labels =  k_means(data)
    rng = np.random.RandomState(42)
    perm = rng.permutation(len(data[1]))
    rand_data, rand_labels = data[1][perm], data[2][perm]
    # rand_score = randIndex(data[2], rand_labels)
    # print rand_score
    unique_scores = set()
    # for i in range(20):
    #     kms_inertia, kms_labels = k_means_centroids(data[1])
    #     rand_score = randIndex(kms_labels, rand_labels)
    #     unique_scores.add(kms_inertia)
    #     print kms_inertia


    cluster_test(sep_data)



if __name__ == '__main__':
    main()

